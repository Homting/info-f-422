{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Statistical foundations of Machine Learning\n",
    "\n",
    "## INFO-F-422 TP: Maximum likelihood and bootstrapping\n",
    "\n",
    "Yann-AÃ«l Le Borgne, Fabrizio Carcillo and Gianluca Bontempi\n",
    "\n",
    "March 28, 2017\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repetition\n",
    "\n",
    "### Basic definitions\n",
    "\n",
    "Let $p_z(z,\\theta)$ be a density function (continuous or discrete) which depends on a parameter $\\theta$ and $D_n=\\{z_1,z_2,\\ldots,z_N\\}$ a sampleset of independent draws following a distribution $p_z$.\n",
    "\n",
    "**Definition 1** The joint probability density function of the sampleset is\n",
    "\n",
    "\\begin{equation}\n",
    " p_{D_N}(D_N,\\theta)=\\prod_{i=1}^N p_z (z_i,\\theta)=L_N(\\theta).\n",
    "\\end{equation}\n",
    "\n",
    "For each fixed $D_N$, $L_N$ is a function of $\\theta$ and is called *empirical likelihood* of $\\theta$ given $D_N$.\n",
    "\n",
    "**Definition 2** Given an unknown parameter $\\theta$ and sampleset $D_N$, the estimation $\\hat{\\theta}$ of $\\theta$ obtained using the maximum likelihood is the value for which the likelihood $L_N(\\theta)$ is maximal:\n",
    "\n",
    "\\begin{equation}\n",
    " \\hat{\\theta}_{ml}=\\arg\\max_{\\theta\\in \\Theta} L_N(\\theta).\n",
    "\\end{equation}\n",
    "\n",
    "Often, the logarithm of the likelihood $l_N(\\theta)$ is used which takes its maximum at the same point as the likelihood $L_N(\\theta)$ since the logarithm is a monotone function:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\hat{\\theta}_{ml}=\\arg\\max_{\\theta\\in \\Theta} L_N(\\theta)=\\arg\\max_{\\theta\\in \\Theta} l_N(\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "The idea of the maximum likelihood method is to find the value of the parameter $\\theta$  for which the observed values have the highest probability of appearing.\n",
    "\n",
    "\n",
    "## Gaussian distribution\n",
    "\n",
    "Given a Gaussian distribution $\\mathbf{z}\\sim\\mathcal{N}(\\mu,\\sigma^2)$, it can be shown that\n",
    "\n",
    "\\begin{align}\n",
    " \\hat{\\mu}_{ml}&=\\frac{\\sum_{i=1}^N z_i}{N}=\\hat{\\mu}\\\\\n",
    "\\hat{\\sigma}^2_{ml}&= \\frac{\\sum_{i=1}^N (z_i-\\hat{\\mu}_{ml})^2}{N}\\neq \\hat{\\sigma}^2.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Let a binary variable (\"coin toss\") take $z$ times the value 1 on $N$ tries. Suppose that the underlying probabilistic model is a binomial variable with unknown parameter $\\theta=p$. The maximum likelihood method gives\n",
    "\\begin{equation}\n",
    " \\hat{p}=\\arg\\max_p\\binom{N}{z}p^z(1-p)^{N-z}.\n",
    "\\end{equation}\n",
    "It can be shown that $\\hat{p}=z/N$. Verify this result empirically by drawing the curve $\\binom{N}{z}p^z(1-p)^{N-z}$ in function of $p$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "The functions `optimize` and `optim` allow, respectively, to carry out numerical optimization (heuristics) of functions of multiple variables. These functions carry out a search for the minimum. Ex.: consider the function $f(x)=(x-1/3)^2$, $x\\in \\,I=[0,1]$. The minimum is given by \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f <- function(x,a) (x-a)^2\n",
    "xmin <- optimize(f, c(0,1), tol=0.0001, a=1/3)\n",
    "xmin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the optimize functions for the estimation of the mean and variance of a variable following a normal distribution applying the maximum likelihood method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "This script demonstrates the bootstrapping method. The bootstrapping allows to evaluate the uncertainty associated to the estimation of a parameter. It consists of resampling the dataset to obtain multiple estimations of the parameter in question. The set of estimations have a distribution which can be used to characterize the uncertainty of the parameter estimation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed(0)\n",
    "\n",
    "Sample.sizes<-seq(100,10000,by=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the given sample sizes, estimate the 0.9 quantile of a uniformly distributed variable in the interval $[0,10]$.\n",
    "Estimate the mean in a second graph followed by an estimation of the variance. \n",
    "\n",
    "Vary the parameter B (B=10 and B=100 for example) to understand how the number of resamplings influence the observed variances.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
